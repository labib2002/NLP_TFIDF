from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import numpy as np
from transformers import pipeline
from tabulate import tabulate


# Function to generate a document using GPT-2 pipeline
def generate_document(phrase):
    generator = pipeline("text-generation", model="gpt2")
    generated_text = generator(phrase, max_length=30, num_return_sequences=1, return_full_text=False)
    return generated_text[0]['generated_text']

# Cleaning function
def clean_text(text):
    # Remove non-alphanumeric characters
    cleaned_text = re.sub(r'[^a-zA-Z\s]', '', text)
    return cleaned_text

# Tokenization and Lemmatization function
def tokenize_and_lemmatize(text):
    tokens = word_tokenize(text)
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]
    return lemmatized_tokens

# Remove stop words
def remove_stopwords(tokens):
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [token for token in tokens if token not in stop_words]
    return filtered_tokens

# Generate documents
phrases = [
    "osama bin laden is considered to be an intersting guy beacuse ",
    "Renewable infinite energy will be",
    "the new laptop released by Asus is "
]

documents = [generate_document(phrase) for phrase in phrases]

# Processing on data
cleaned_documents = [clean_text(doc) for doc in documents]
tokenized_documents = [tokenize_and_lemmatize(doc) for doc in cleaned_documents]
normalized_documents = [" ".join(tokens) for tokens in tokenized_documents]

# TF-IDF calculation using scikit-learn
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix_sklearn = tfidf_vectorizer.fit_transform(normalized_documents)
feature_names = tfidf_vectorizer.get_feature_names_out()

# Print TF-IDF results (scikit-learn)
print("\nTF-IDF Results (scikit-learn):")
for i, doc in enumerate(documents):
    print("\nDocument:", phrases[i])
    print("Processed Document:", normalized_documents[i])
    print("TF-IDF Vector:")
    tfidf_values = []
    for j, feature in enumerate(feature_names):
        tfidf_value = tfidf_matrix_sklearn[i, j]
        if tfidf_value > 0:
            tfidf_values.append([feature, tfidf_value])
    print(tabulate(tfidf_values, headers=["Feature", "TF-IDF"], tablefmt="pretty"))

# Normalized TF-IDF
normalized_tfidf_matrix = normalize(tfidf_matrix_sklearn, norm='l2', axis=1)

from scratch:
def calculate_tf(document):
    word_counts = {}
    for word in document.split():
        word_counts[word] = word_counts.get(word, 0) + 1
    total_words = len(document.split())
    tf_dict = {word: count / total_words for word, count in word_counts.items()}
    return tf_dict

def calculate_idf(documents):
    num_documents = len(documents)
    word_doc_counts = {}
    for document in documents:
        for word in set(document.split()):
            word_doc_counts[word] = word_doc_counts.get(word, 0) + 1
    idf_dict = {word: np.log(num_documents / (count + 1)) for word, count in word_doc_counts.items()}
    return idf_dict

def calculate_tfidf(documents):
    idf_dict = calculate_idf(documents)
    tfidf_matrix = []
    for document in documents:
        tf_dict = calculate_tf(document)
        tfidf_vector = [(1 + np.log(tf_dict.get(word, 0))) * idf_dict.get(word, 0) for word in idf_dict]
        tfidf_matrix.append(tfidf_vector)
    return np.array(tfidf_matrix)


# Print TF-IDF results (from scratch)
print("\nTF-IDF Results (From Scratch with Log):")
for i, doc in enumerate(documents):
    print("\nDocument:", phrases[i])
    print("Processed Document:", normalized_documents[i])
    print("TF-IDF Vector:")

    # Calculate TF and IDF on the fly for the current document
    tf_dict = calculate_tf(normalized_documents[i])
    idf_dict = calculate_idf(normalized_documents)
#github + idf sklearn 
    tfidf_values = []
    for j, feature in enumerate(feature_names):
        tfidf_value = calculate_tfidf(normalized_documents)[i, j]
        if tfidf_value > 0:
            tfidf_values.append([feature, tfidf_value])
            # Print TF and IDF values for each feature with log in TF
            print(feature, "- TF:", (1 + np.log(tf_dict.get(feature, 0))), "- IDF:", idf_dict[feature])

    print(tabulate(tfidf_values, headers=["Feature", "TF-IDF"], tablefmt="pretty"))
